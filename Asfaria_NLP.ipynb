{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a21e12-74ad-40a4-9bca-b5a10321a363",
   "metadata": {},
   "source": [
    "# SECTION 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18eaa2-c88a-4635-bb8a-4f38f21a00be",
   "metadata": {},
   "source": [
    "## SECTION 1.1: Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a3724-ea2d-4191-8dfd-152757c28b1d",
   "metadata": {},
   "source": [
    "Corpus contains raw text (ASCII/UTF-8) and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a8bf51-d26c-48f6-b6d5-5a02ac6cec38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a29fcf-8dc1-4d21-a749-6fc19a1ef949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e14862-21a7-44f3-84cd-baf3ce0d3832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16eed9e3-53af-4c91-836b-65b17f9b01d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1b4b648-19ea-419a-a86b-b5700c034f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534e008-50f4-409e-8ccf-e1cc86187857",
   "metadata": {},
   "source": [
    "## SECTION 1.2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb577-c962-443f-a4d3-97b3654f1ddd",
   "metadata": {},
   "source": [
    "2 types of words:\n",
    "1. content words\n",
    "2. stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b587d95-cc23-4c8d-9fbb-8587d6ffcd67",
   "metadata": {},
   "source": [
    "Pure Python, spaCy, or NLTK can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df5ef738-55de-4f2f-b983-c814c7bd31de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "935fa120-90c4-499d-a87e-5be6ddf6c9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b63249-a33f-4fd1-a608-3d187a9645d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "text = \"Mary, don't slap the green witch\"\n",
    "print(\n",
    "    [\n",
    "        str(token) for token\n",
    "        in nlp(text.lower())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbede121-5a19-4211-9f5c-715ec9153c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import \\\n",
    "TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ead497-73a8-4a42-8575-93a21b7c6c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"Snow White and the Seven Degrees #MakeAMovieCold @midnight :-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(\n",
    "    tokenizer.tokenize(tweet.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90ef25-ba70-414d-bd2d-ba8d688165ba",
   "metadata": {},
   "source": [
    "NLTK tweet tokenizer preserves hashtags, handles, and smiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d3324-9d4b-443e-9e15-656f54655b1a",
   "metadata": {},
   "source": [
    "## SECTION 1.3: WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafde385-53a0-4297-9411-c2e22bae6978",
   "metadata": {},
   "source": [
    "WordNet is a large lexical database in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9812f63b-22c4-4eba-9480-e83de161dc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c84467c-58ee-47a6-ab34-e1a9336440ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'secure', 'unspoiled', 'skilful', 'thoroughly', 'right', 'unspoilt', 'practiced', 'honest', 'upright', 'commodity', 'goodness', 'adept', 'estimable', 'effective', 'beneficial', 'trade_good', 'good', 'expert', 'full', 'safe', 'salutary', 'honorable', 'just', 'soundly', 'in_force', 'dear', 'well', 'ripe', 'serious', 'sound', 'proficient', 'undecomposed', 'near', 'skillful', 'respectable', 'dependable', 'in_effect'}\n",
      "{'evil', 'badness', 'ill', 'evilness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('good'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(\n",
    "                l.antonyms()[0].name()\n",
    "            )\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d07f5-f65f-45b9-beb8-e68840007138",
   "metadata": {},
   "source": [
    "## SECTION 1.4: Grammartical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca4e5a22-a375-478b-89fd-fc17cc64d166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb506b08-6191-42a6-971f-de246e942662",
   "metadata": {},
   "source": [
    "## SECTION 1.5: Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d7baabb-9911-4e34-ae14-fcb039d72cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e88fdd59-400f-478d-9290-175c07fd14c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a4565996c4514fceb6a97aac7b0a2282-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,47.0 225.0,47.0 225.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,92.0 310.0,92.0 310.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310.0,139.0 L318.0,127.0 302.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,139.0 L408.0,127.0 392.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-4\" stroke-width=\"2px\" d=\"M430,137.0 C430,92.0 490.0,92.0 490.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M490.0,139.0 L498.0,127.0 482.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-5\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 585.0,47.0 585.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M585.0,139.0 L593.0,127.0 577.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-6\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670.0,139.0 L678.0,127.0 662.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-7\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,139.0 L782,127.0 798,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a4565996c4514fceb6a97aac7b0a2282-0-9\" stroke-width=\"2px\" d=\"M700,137.0 C700,2.0 950.0,2.0 950.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a4565996c4514fceb6a97aac7b0a2282-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,139.0 L958.0,127.0 942.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, \n",
    "                style='dep', \n",
    "                jupyter='True', \n",
    "                options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a276b9-be34-4434-93b3-03e41b69d47e",
   "metadata": {},
   "source": [
    "## SECTION 1.6: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "650f1ac1-c435-4608-9162-5f7d86e8b05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I just bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9 am\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " because the stock went up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    30%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    just 2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    WSJ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    'I just bought 2 shares at 9 am because the stock went up 30% in just 2 days according to the WSJ'\n",
    ")\n",
    "displacy.render(doc,\n",
    "              style='ent',\n",
    "              jupyter='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851ab16-a55a-4168-b5a6-8dbe0bfbea35",
   "metadata": {},
   "source": [
    "# SECTION 2: Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976c3f1-84ad-4fdd-be07-4322c87fff04",
   "metadata": {},
   "source": [
    "**Why is representation important?**\n",
    "\n",
    "Text representation scheme must facilitate the extraction of the features.\n",
    "\n",
    "The *semantics* (meaning) of a sentence comes from the 4 steps:\n",
    "1. Break the sentence into lexical units\n",
    "2. Derive the meaning of each unit\n",
    "3. Understand the syntactic (grammatical) structure of the sentence\n",
    "4. Understand the context in which the sentence appears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768f772-e30c-438a-8039-99916c5b09fc",
   "metadata": {},
   "source": [
    "**What is text representation?**\n",
    "\n",
    "Text representation is the conversion from of raw text into a suitable numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e33f-2755-4d6f-9609-67bf33be6b5b",
   "metadata": {},
   "source": [
    "**Legacy Techniques**\n",
    "1. one-hot encoding\n",
    "2. bag of words\n",
    "3. n-gram\n",
    "4. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb65afd-7439-4803-a645-83569c51291f",
   "metadata": {},
   "source": [
    "## SECTION 2.1: One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05c8ef-93fa-4c90-b6af-a740c002bc6e",
   "metadata": {},
   "source": [
    "1. No information about words relations\n",
    "2. Must pre-determine vocabulary size\n",
    "3. Size of input vector scales with size of vocabulary\n",
    "4. \"Out-of-vocabulary\" (OOV) problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c9d37d5-e63a-46db-8b9a-8d665db7533f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "921c3272-1aff-46b6-9422-74a47e7b63df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def one_hot(word, word_dict):\n",
    "    vector = np.zeros(len(word_dict))\n",
    "    # vector[word_dict[word]] = 1\n",
    "    if word in word_dict:\n",
    "        vector[word_dict[word]] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "words = ['rome', 'paris', 'italy', 'france']\n",
    "word_dict = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "print(one_hot(\"paris\", word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ef398-19b6-4ec7-b45c-f12557de9385",
   "metadata": {},
   "source": [
    "## SECTION 2.2: Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d16db-44c9-4ec0-9a01-5698a9a80665",
   "metadata": {},
   "source": [
    "Bag of words is a vector representation of a text produced by simply adding up all the one-hot encoded vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f8f4b-72c8-403d-a1bf-c8a7b3bf66e6",
   "metadata": {},
   "source": [
    "1. Vectors simply contain the number of times each word appears in our document.\n",
    "2. *Orderless*\n",
    "3. No notion of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "703f9381-3560-46c3-8a8f-6ab2cf4ad7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 50\n",
    "\n",
    "text_words = [\n",
    "    'rome', 'paris', 'italy', 'france',\n",
    "    'rome', 'magnificent', 'tourism', 'night',\n",
    "    'tourism', 'tourism'\n",
    "]\n",
    "\n",
    "# bow = np.zeros(vocabulary_size)\n",
    "bow = [0] * len(word_dict)\n",
    "\n",
    "for word in text_words:\n",
    "    hot_word = one_hot(word, word_dict)\n",
    "    # bow += hot_word\n",
    "    bow = [sum(x) for x in zip(bow, hot_word)]  # Element-wise sum\n",
    "\n",
    "print(bow)\n",
    "\n",
    "bow[word_dict[\"paris\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b679f-1283-46a6-85e6-47b0fa8ef74e",
   "metadata": {},
   "source": [
    "## SECTION 2.3: N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4444910-4a14-4eb0-a40a-2a67a5228cc0",
   "metadata": {},
   "source": [
    "N-gram model is a contiguous sequence of n items from a given sample of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee83349-5ffa-4677-a0f1-e6b9cc9b5e70",
   "metadata": {},
   "source": [
    "1. Vocabulary = set of all n-grams in corpus\n",
    "2. No notion of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02eb6512-36f7-4991-889e-7c5427c45822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Machines', 'take', 'me', 'by', 'surprise')\n",
      "('take', 'me', 'by', 'surprise', 'with')\n",
      "('me', 'by', 'surprise', 'with', 'great')\n",
      "('by', 'surprise', 'with', 'great', 'frequency')\n",
      "('surprise', 'with', 'great', 'frequency', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "text = \"Machines take me by surprise with great frequency.\"\n",
    "\n",
    "n = 5\n",
    "pentagrams = ngrams(nltk.word_tokenize(text), n)\n",
    "\n",
    "for grams in pentagrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24e787-48d7-4ec7-aff4-63962a65366f",
   "metadata": {},
   "source": [
    "## SECTION 2.4: Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa2163-1b50-4010-ad6f-ca1aa8b99dc5",
   "metadata": {},
   "source": [
    "A collocation is a sequence of words that occur together unusually often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbb7a3-b0e6-4f29-95d7-63f097679beb",
   "metadata": {},
   "source": [
    "`nltk.collocations` can help identifying phrases that act like single words.\n",
    "\n",
    "In the example below, bi-grams are paired with a \"more likely to occur\" score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67acda6a-109e-4939-b0aa-9861e4f246b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New [('York', 4634.968955894195), ('Orleans', 611.6951040864856), ('England', 557.5789255397682), ('Jersey', 265.2781409189113), ('Testament', 182.6595658588261)]\n"
     ]
    }
   ],
   "source": [
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "import collections\n",
    "\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.brown.words()\n",
    ")\n",
    "scored = finder.score_ngrams(bgm.likelihood_ratio)\n",
    "\n",
    "# Group bigrams by first word in bigram\n",
    "prefix_keys = collections.defaultdict(list)\n",
    "for key, scores in scored:\n",
    "    prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "# Sorted key bigrams by strongest association\n",
    "for key in prefix_keys:\n",
    "    prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "print('New', prefix_keys['New'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932298d3-c53f-4070-9634-5b379368cf16",
   "metadata": {},
   "source": [
    "## SECTION 2.5: Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6a18ae3-b36a-43d9-b5ee-2191405903ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45e34cab-c0c9-40d7-95c9-bc388ef60bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 26\n",
      "a 14\n",
      "and 10\n",
      "their 7\n",
      "of 6\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "'''\n",
    "for line in gzip.open(\"F:/EDGE/LearningAI/sample_text.txt\", 'rt'):\n",
    "    data.extend(line.strip().split())\n",
    "'''\n",
    "\n",
    "for line in open(\"F:/EDGE/LearningAI/sample_text.txt\", 'rt'):\n",
    "    data.extend(line.strip().split())\n",
    "\n",
    "'''\n",
    "# Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the file and split the text into words\n",
    "    words = file.read().split()\n",
    "'''\n",
    "\n",
    "counts = Counter(data)\n",
    "\n",
    "sorted_counts = sorted(list(counts.items()), key=lambda x:x[1], \n",
    "                       reverse=True)\n",
    "\n",
    "for word, count in sorted_counts[:5]:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14ed0630-13ee-43d7-a0d7-a0c443de0505",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 26), ('a', 14), ('and', 10), ('their', 7), ('of', 6)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "all_words = nltk.FreqDist(data)\n",
    "print(all_words.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2737b6f-7b92-48c7-a086-22c98af18b90",
   "metadata": {},
   "source": [
    "## SECTION 2.6: 3 Ways to Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be64ba3-0f89-4d39-9f5b-7affd9e2abbc",
   "metadata": {},
   "source": [
    "**(1/3) Remove the most common 100 words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2f99ee5-e82b-4df8-815d-6e04fcd76855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = set(\n",
    "    [\n",
    "        word for word,\n",
    "        count in sorted_counts[:100]\n",
    "    ]\n",
    ")\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for word in data:\n",
    "    if word not in stopwords:\n",
    "        clean_data.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ae6fd-d8c1-4235-8879-f64decb54cb8",
   "metadata": {},
   "source": [
    "**(2/3) Use nltk predefined stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3e6907f-1d41-4aa3-ba3a-15e344e7d192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 179\n",
      "First five stop words: ['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "print('Number of stopwords: %d' % len(nltk_stopwords))\n",
    "print('First five stop words: %s' % list(nltk_stopwords)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0981-6129-4f24-8557-ab2bd9ed31fb",
   "metadata": {},
   "source": [
    "**(3/3) Use spaCy predefined stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6909c612-c68a-4b9a-892e-6356f3e45d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 326\n",
      "First five stop words: ['thru', 'very', '‘ll', 'them', '‘s']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
    "print('First five stop words: %s' % list(spacy_stopwords)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1159f874-6ca0-4897-a837-a6f951decd28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GENERATED', 'CHATGPT', 'time', ',', 'remote', 'island', 'nestled', 'heart', 'Pacific', ',', 'adventurer', 'named', 'Alex', 'found', 'stranded', 'unexpected', 'shipwreck', '.', 'island', ',', 'lush', 'vibrant', 'foliage', 'adorned', 'pristine', 'beaches', ',', 'like', 'paradise', 'glance', '.', ',', 'Alex', 'soon', 'realized', 'beauty', 'concealed', 'challenges', 'lay', 'ahead', '.', 'days', ',', 'Alex', 'scoured', 'shoreline', 'salvageable', 'items', 'shipwreck', '.', 'debris', ',', 'found', 'crates', 'canned', 'food', ',', 'tattered', 'functional', 'tent', ',', 'waterproof', 'box', 'containing', 'matches', '.', 'resources', ',', 'established', 'small', 'camp', 'near', 'freshwater', 'stream', ',', 'ensuring', 'basic', 'needs', 'met', '.', 'Days', 'turned', 'weeks', ',', 'Alex', 'survival', 'instincts', 'kicked', '.', 'began', 'exploring', 'island', 'interior', ',', 'learning', 'identify', 'edible', 'plants', 'honing', 'fishing', 'skills', '.', 'way', ',', 'encountered', 'peculiar', 'wildlife', ',', 'friendly', '.', 'encounter', 'added', 'understanding', 'island', 'ecosystem', '.', 'Loneliness', 'soon', 'companion', ',', 'combat', ',', 'Alex', 'crafted', 'makeshift', 'journal', '.', 'documented', 'experiences', ',', 'pouring', 'hopes', 'fears', 'weathered', 'pages', '.', 'journal', 'confidant', ',', 'way', 'preserve', 'sanity', 'solitude', 'island', '.', 'Amidst', 'challenges', ',', 'Alex', 'discovered', 'hidden', 'cave', 'adorned', 'ancient', 'markings', 'artifacts', ',', 'hinting', 'island', 'mysterious', 'past', '.', 'markings', 'told', 'story', 'lost', 'civilization', 'called', 'island', 'home', '.', 'Intrigued', ',', 'Alex', 'delved', 'deeper', 'cave', ',', 'unearthing', 'secrets', 'forever', 'change', 'perspective', 'island', '.', 'end', ',', 'Alex', 'journey', 'stranded', 'island', 'survival', 'self', '-', 'discovery', 'resilience', 'human', 'spirit', '.', 'repaired', 'makeshift', 'raft', 'set', 'sail', 'horizon', ',', 'carried', 'island', 'lessons', ',', 'forever', 'changed', 'adventure', 'fate', 'cast', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Combine the list of words into a single string\n",
    "data_str = ' '.join(data)\n",
    "\n",
    "# Process your text using spaCy\n",
    "doc = nlp(data_str)\n",
    "\n",
    "# Extract tokens, excluding stop words\n",
    "tokens = [token.text for token in doc \n",
    "          if not nlp.vocab[token.text].is_stop]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d771cce9-8142-4adb-88a3-52d44f1b361e",
   "metadata": {},
   "source": [
    "## SECTION 2.7: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac572f-6ce2-4b89-a9d9-cb941be5d3d7",
   "metadata": {},
   "source": [
    "TF-IDF reflects how important a word is to a document in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00446cc4-afd9-4d70-97e1-f0bb8e7c1b17",
   "metadata": {},
   "source": [
    "$$W_{x,y}=tf_{x,y}\\times log(\\frac{N}{df_{x}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3376846-3620-4e63-95ac-43b838bcf627",
   "metadata": {},
   "source": [
    "Here,\n",
    "> $W_{x,y}$ is the TF-IDF, i.e. the term $x$ within the document $y$\n",
    "\n",
    "> $tf_{x,y}$ is the frequency of $x$ in $y$\n",
    "\n",
    "> $df_{x}$ is the number of documents containing $x$\n",
    "\n",
    "> $N$ is the total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6884c68-e441-4a0a-802e-877c40c1f9d2",
   "metadata": {},
   "source": [
    "TF-IDF of common words is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1078f-cd80-43f1-8f19-b2a6c25881fe",
   "metadata": {},
   "source": [
    "**(1/3) TF-IDF from Scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83fa4cef-c4e8-43f7-8c92-fe96e3f91759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ba9e9-6ebf-4bb7-a815-457f83cb4099",
   "metadata": {},
   "source": [
    "**(2/3) TF-IDF using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cd70c94-1b7a-4117-b39f-a41599c08733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.42471718586982765\n",
      "  (0, 4)\t0.30218977576862155\n",
      "  (0, 1)\t0.30218977576862155\n",
      "  (0, 3)\t0.30218977576862155\n",
      "  (0, 0)\t0.42471718586982765\n",
      "  (0, 6)\t0.6043795515372431\n",
      "  (1, 2)\t0.42471718586982765\n",
      "  (1, 7)\t0.42471718586982765\n",
      "  (1, 4)\t0.30218977576862155\n",
      "  (1, 1)\t0.30218977576862155\n",
      "  (1, 3)\t0.30218977576862155\n",
      "  (1, 6)\t0.6043795515372431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "D1 = \"The car is driven on the road.\"\n",
    "D2 = \"The truck is driven on the highway.\"\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "response = vectorizer.fit_transform([D1, D2])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44ce1c74-3131-4592-8990-dc93fdf652c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(list(enumerate(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731a086-0506-4727-8a9b-12e58ea3cbe6",
   "metadata": {},
   "source": [
    "`fit` learns vocabulary and idf from the training set.\n",
    "\n",
    "`transform` transforms documents to document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5321e-28da-43c1-a0d5-bee25812b0db",
   "metadata": {},
   "source": [
    "**(3/3) TF_IDF using Sklearn + StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa5add55-6859-4b64-a846-3a2aead80efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9434e556-ea5e-4131-9c3e-e61c6fc62683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.6316672017376245\n",
      "  (0, 1)\t0.4494364165239821\n",
      "  (0, 0)\t0.6316672017376245\n",
      "  (1, 2)\t0.6316672017376245\n",
      "  (1, 4)\t0.6316672017376245\n",
      "  (1, 1)\t0.4494364165239821\n",
      "[(0, 'car'), (1, 'driven'), (2, 'highway'), (3, 'road'), (4, 'truck')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npprint(\\n    list(\\n        enumerate(\\n            vectorizer.get_feature_names()\\n        )\\n    )\\n)\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "D1 = \"The car is driven on the road.\"\n",
    "D2 = \"The truck is driven on the highway.\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "response = vectorizer.fit_transform([D1, D2])\n",
    "\n",
    "print(response)\n",
    "\n",
    "# Print the feature names\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "pprint(list(enumerate(feature_names)))\n",
    "'''\n",
    "pprint(\n",
    "    list(\n",
    "        enumerate(\n",
    "            vectorizer.get_feature_names()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39558322-0d20-4e5b-a42a-9768ddb9d90a",
   "metadata": {},
   "source": [
    "# SECTION 3: Vector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99231387-f394-4c05-9dbe-9a68f9f4beff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itil",
   "language": "python",
   "name": "itil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
