{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38a21e12-74ad-40a4-9bca-b5a10321a363",
      "metadata": {
        "id": "38a21e12-74ad-40a4-9bca-b5a10321a363"
      },
      "source": [
        "# SECTION 1: Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d18eaa2-c88a-4635-bb8a-4f38f21a00be",
      "metadata": {
        "id": "3d18eaa2-c88a-4635-bb8a-4f38f21a00be"
      },
      "source": [
        "## SECTION 1.1: Corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285a3724-ea2d-4191-8dfd-152757c28b1d",
      "metadata": {
        "id": "285a3724-ea2d-4191-8dfd-152757c28b1d"
      },
      "source": [
        "Corpus contains raw text (ASCII/UTF-8) and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "98a8bf51-d26c-48f6-b6d5-5a02ac6cec38",
      "metadata": {
        "tags": [],
        "id": "98a8bf51-d26c-48f6-b6d5-5a02ac6cec38"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a5a29fcf-8dc1-4d21-a749-6fc19a1ef949",
      "metadata": {
        "tags": [],
        "id": "a5a29fcf-8dc1-4d21-a749-6fc19a1ef949"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f5e14862-21a7-44f3-84cd-baf3ce0d3832",
      "metadata": {
        "tags": [],
        "id": "f5e14862-21a7-44f3-84cd-baf3ce0d3832"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "16eed9e3-53af-4c91-836b-65b17f9b01d3",
      "metadata": {
        "tags": [],
        "id": "16eed9e3-53af-4c91-836b-65b17f9b01d3"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzMMapVadhOq",
        "outputId": "44d3c807-5c45-45aa-f948-0af0482bdddf"
      },
      "id": "EzMMapVadhOq",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e1b4b648-19ea-419a-a86b-b5700c034f1d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1b4b648-19ea-419a-a86b-b5700c034f1d",
        "outputId": "f8a0e156-7b25-41fa-8ed2-ce137f1417e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "brown.categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a534e008-50f4-409e-8ccf-e1cc86187857",
      "metadata": {
        "id": "a534e008-50f4-409e-8ccf-e1cc86187857"
      },
      "source": [
        "## SECTION 1.2: Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5cb577-c962-443f-a4d3-97b3654f1ddd",
      "metadata": {
        "id": "9f5cb577-c962-443f-a4d3-97b3654f1ddd"
      },
      "source": [
        "2 types of words:\n",
        "1. content words\n",
        "2. stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b587d95-cc23-4c8d-9fbb-8587d6ffcd67",
      "metadata": {
        "id": "0b587d95-cc23-4c8d-9fbb-8587d6ffcd67"
      },
      "source": [
        "Pure Python, spaCy, or NLTK can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df5ef738-55de-4f2f-b983-c814c7bd31de",
      "metadata": {
        "tags": [],
        "id": "df5ef738-55de-4f2f-b983-c814c7bd31de"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "935fa120-90c4-499d-a87e-5be6ddf6c9e0",
      "metadata": {
        "tags": [],
        "id": "935fa120-90c4-499d-a87e-5be6ddf6c9e0"
      },
      "outputs": [],
      "source": [
        "import en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "65b63249-a33f-4fd1-a608-3d187a9645d2",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65b63249-a33f-4fd1-a608-3d187a9645d2",
        "outputId": "015c8dca-be40-42ae-9cfe-db17af107647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
          ]
        }
      ],
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "text = \"Mary, don't slap the green witch\"\n",
        "print(\n",
        "    [\n",
        "        str(token) for token\n",
        "        in nlp(text.lower())\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bbede121-5a19-4211-9f5c-715ec9153c7c",
      "metadata": {
        "tags": [],
        "id": "bbede121-5a19-4211-9f5c-715ec9153c7c"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import \\\n",
        "TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76ead497-73a8-4a42-8575-93a21b7c6c6f",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ead497-73a8-4a42-8575-93a21b7c6c6f",
        "outputId": "c9e15b10-af7a-42a8-c602-d2b1c5c0e062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
          ]
        }
      ],
      "source": [
        "tweet = \"Snow White and the Seven Degrees #MakeAMovieCold @midnight :-)\"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(\n",
        "    tokenizer.tokenize(tweet.lower())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d90ef25-ba70-414d-bd2d-ba8d688165ba",
      "metadata": {
        "id": "3d90ef25-ba70-414d-bd2d-ba8d688165ba"
      },
      "source": [
        "NLTK tweet tokenizer preserves hashtags, handles, and smiles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685d3324-9d4b-443e-9e15-656f54655b1a",
      "metadata": {
        "id": "685d3324-9d4b-443e-9e15-656f54655b1a"
      },
      "source": [
        "## SECTION 1.3: WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafde385-53a0-4297-9411-c2e22bae6978",
      "metadata": {
        "id": "dafde385-53a0-4297-9411-c2e22bae6978"
      },
      "source": [
        "WordNet is a large lexical database in English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9812f63b-22c4-4eba-9480-e83de161dc72",
      "metadata": {
        "tags": [],
        "id": "9812f63b-22c4-4eba-9480-e83de161dc72"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY7_3fyrdttQ",
        "outputId": "39207317-20d9-4c6c-caa4-60e2ed50dd60"
      },
      "id": "YY7_3fyrdttQ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5c84467c-58ee-47a6-ab34-e1a9336440ec",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c84467c-58ee-47a6-ab34-e1a9336440ec",
        "outputId": "bd482c2e-a2a3-4793-8e91-d89098cc1307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'honorable', 'soundly', 'goodness', 'just', 'upright', 'commodity', 'proficient', 'adept', 'in_force', 'thoroughly', 'skilful', 'undecomposed', 'respectable', 'unspoilt', 'well', 'near', 'salutary', 'beneficial', 'dependable', 'ripe', 'honest', 'good', 'serious', 'unspoiled', 'safe', 'full', 'expert', 'skillful', 'trade_good', 'sound', 'dear', 'estimable', 'effective', 'in_effect', 'secure', 'practiced', 'right'}\n",
            "{'evil', 'ill', 'evilness', 'bad', 'badness'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets('good'):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(\n",
        "                l.antonyms()[0].name()\n",
        "            )\n",
        "\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6d07f5-f65f-45b9-beb8-e68840007138",
      "metadata": {
        "id": "7d6d07f5-f65f-45b9-beb8-e68840007138"
      },
      "source": [
        "## SECTION 1.4: Grammartical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ca4e5a22-a375-478b-89fd-fc17cc64d166",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca4e5a22-a375-478b-89fd-fc17cc64d166",
        "outputId": "55cb6ec2-19e7-4554-d3da-430466259ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.K. PROPN dobj\n",
            "startup NOUN dep\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "1 NUM compound\n",
            "billion NUM pobj\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb506b08-6191-42a6-971f-de246e942662",
      "metadata": {
        "id": "cb506b08-6191-42a6-971f-de246e942662"
      },
      "source": [
        "## SECTION 1.5: Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9d7baabb-9911-4e34-ae14-fcb039d72cf8",
      "metadata": {
        "tags": [],
        "id": "9d7baabb-9911-4e34-ae14-fcb039d72cf8"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e88fdd59-400f-478d-9290-175c07fd14c6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "e88fdd59-400f-478d-9290-175c07fd14c6",
        "outputId": "cbac56bb-3adc-4b68-d0e1-9c2fd70f36c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a25f9d46b5bf4507b4bf0b1c5be7fe33-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">looking</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">at</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">buying</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">startup</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">1</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">billion</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,47.0 225.0,47.0 225.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,92.0 310.0,92.0 310.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310.0,139.0 L318.0,127.0 302.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400.0,139.0 L408.0,127.0 392.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-4\" stroke-width=\"2px\" d=\"M430,137.0 C430,92.0 490.0,92.0 490.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M490.0,139.0 L498.0,127.0 482.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-5\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 585.0,47.0 585.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M585.0,139.0 L593.0,127.0 577.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-6\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M670.0,139.0 L678.0,127.0 662.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-7\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,139.0 L782,127.0 798,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-9\" stroke-width=\"2px\" d=\"M700,137.0 C700,2.0 950.0,2.0 950.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-a25f9d46b5bf4507b4bf0b1c5be7fe33-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M950.0,139.0 L958.0,127.0 942.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc,\n",
        "                style='dep',\n",
        "                jupyter='True',\n",
        "                options={'distance':90})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a276b9-be34-4434-93b3-03e41b69d47e",
      "metadata": {
        "id": "37a276b9-be34-4434-93b3-03e41b69d47e"
      },
      "source": [
        "## SECTION 1.6: Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "650f1ac1-c435-4608-9162-5f7d86e8b05d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "650f1ac1-c435-4608-9162-5f7d86e8b05d",
        "outputId": "f5f4dc90-d211-46ce-9914-dd18c2ba0c03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I just bought \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " shares at \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    9 am\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " because the stock went up \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    30%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    just 2 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " according to the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    WSJ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "doc = nlp(\n",
        "    'I just bought 2 shares at 9 am because the stock went up 30% in just 2 days according to the WSJ'\n",
        ")\n",
        "displacy.render(doc,\n",
        "              style='ent',\n",
        "              jupyter='True')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5851ab16-a55a-4168-b5a6-8dbe0bfbea35",
      "metadata": {
        "id": "5851ab16-a55a-4168-b5a6-8dbe0bfbea35"
      },
      "source": [
        "# SECTION 2: Text Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a976c3f1-84ad-4fdd-be07-4322c87fff04",
      "metadata": {
        "id": "a976c3f1-84ad-4fdd-be07-4322c87fff04"
      },
      "source": [
        "**Why is representation important?**\n",
        "\n",
        "Text representation scheme must facilitate the extraction of the features.\n",
        "\n",
        "The *semantics* (meaning) of a sentence comes from the 4 steps:\n",
        "1. Break the sentence into lexical units\n",
        "2. Derive the meaning of each unit\n",
        "3. Understand the syntactic (grammatical) structure of the sentence\n",
        "4. Understand the context in which the sentence appears"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1768f772-e30c-438a-8039-99916c5b09fc",
      "metadata": {
        "id": "1768f772-e30c-438a-8039-99916c5b09fc"
      },
      "source": [
        "**What is text representation?**\n",
        "\n",
        "Text representation is the conversion from of raw text into a suitable numerical form."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f4e33f-2755-4d6f-9609-67bf33be6b5b",
      "metadata": {
        "id": "30f4e33f-2755-4d6f-9609-67bf33be6b5b"
      },
      "source": [
        "**Legacy Techniques**\n",
        "1. one-hot encoding\n",
        "2. bag of words\n",
        "3. n-gram\n",
        "4. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb65afd-7439-4803-a645-83569c51291f",
      "metadata": {
        "id": "3cb65afd-7439-4803-a645-83569c51291f"
      },
      "source": [
        "## SECTION 2.1: One-hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e05c8ef-93fa-4c90-b6af-a740c002bc6e",
      "metadata": {
        "id": "0e05c8ef-93fa-4c90-b6af-a740c002bc6e"
      },
      "source": [
        "1. No information about words relations\n",
        "2. Must pre-determine vocabulary size\n",
        "3. Size of input vector scales with size of vocabulary\n",
        "4. \"Out-of-vocabulary\" (OOV) problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1c9d37d5-e63a-46db-8b9a-8d665db7533f",
      "metadata": {
        "tags": [],
        "id": "1c9d37d5-e63a-46db-8b9a-8d665db7533f"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "921c3272-1aff-46b6-9422-74a47e7b63df",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "921c3272-1aff-46b6-9422-74a47e7b63df",
        "outputId": "cd358858-88e8-4dde-c776-153a51b2dfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "def one_hot(word, word_dict):\n",
        "    vector = np.zeros(len(word_dict))\n",
        "    # vector[word_dict[word]] = 1\n",
        "    if word in word_dict:\n",
        "        vector[word_dict[word]] = 1\n",
        "\n",
        "    return vector\n",
        "\n",
        "words = ['rome', 'paris', 'italy', 'france']\n",
        "word_dict = {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "print(one_hot(\"paris\", word_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0ef398-19b6-4ec7-b45c-f12557de9385",
      "metadata": {
        "id": "7e0ef398-19b6-4ec7-b45c-f12557de9385"
      },
      "source": [
        "## SECTION 2.2: Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782d16db-44c9-4ec0-9a01-5698a9a80665",
      "metadata": {
        "id": "782d16db-44c9-4ec0-9a01-5698a9a80665"
      },
      "source": [
        "Bag of words is a vector representation of a text produced by simply adding up all the one-hot encoded vectors:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "197f8f4b-72c8-403d-a1bf-c8a7b3bf66e6",
      "metadata": {
        "id": "197f8f4b-72c8-403d-a1bf-c8a7b3bf66e6"
      },
      "source": [
        "1. Vectors simply contain the number of times each word appears in our document.\n",
        "2. *Orderless*\n",
        "3. No notion of similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "703f9381-3560-46c3-8a8f-6ab2cf4ad7d2",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "703f9381-3560-46c3-8a8f-6ab2cf4ad7d2",
        "outputId": "a9308b4b-c131-4e7c-c5f0-09eccadc6cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.0, 1.0, 1.0, 1.0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "vocabulary_size = 50\n",
        "\n",
        "text_words = [\n",
        "    'rome', 'paris', 'italy', 'france',\n",
        "    'rome', 'magnificent', 'tourism', 'night',\n",
        "    'tourism', 'tourism'\n",
        "]\n",
        "\n",
        "# bow = np.zeros(vocabulary_size)\n",
        "bow = [0] * len(word_dict)\n",
        "\n",
        "for word in text_words:\n",
        "    hot_word = one_hot(word, word_dict)\n",
        "    # bow += hot_word\n",
        "    bow = [sum(x) for x in zip(bow, hot_word)]  # Element-wise sum\n",
        "\n",
        "print(bow)\n",
        "\n",
        "bow[word_dict[\"paris\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385b679f-1283-46a6-85e6-47b0fa8ef74e",
      "metadata": {
        "id": "385b679f-1283-46a6-85e6-47b0fa8ef74e"
      },
      "source": [
        "## SECTION 2.3: N-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4444910-4a14-4eb0-a40a-2a67a5228cc0",
      "metadata": {
        "id": "e4444910-4a14-4eb0-a40a-2a67a5228cc0"
      },
      "source": [
        "N-gram model is a contiguous sequence of n items from a given sample of text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee83349-5ffa-4677-a0f1-e6b9cc9b5e70",
      "metadata": {
        "id": "3ee83349-5ffa-4677-a0f1-e6b9cc9b5e70"
      },
      "source": [
        "1. Vocabulary = set of all n-grams in corpus\n",
        "2. No notion of similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2682k9Fd67h",
        "outputId": "b19e800e-29dd-439a-9e4e-2d74b850c0d8"
      },
      "id": "l2682k9Fd67h",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "02eb6512-36f7-4991-889e-7c5427c45822",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02eb6512-36f7-4991-889e-7c5427c45822",
        "outputId": "5501741e-18f3-4f90-ccd7-579e5db2e086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Machines', 'take', 'me', 'by', 'surprise')\n",
            "('take', 'me', 'by', 'surprise', 'with')\n",
            "('me', 'by', 'surprise', 'with', 'great')\n",
            "('by', 'surprise', 'with', 'great', 'frequency')\n",
            "('surprise', 'with', 'great', 'frequency', '.')\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "text = \"Machines take me by surprise with great frequency.\"\n",
        "\n",
        "n = 5\n",
        "pentagrams = ngrams(nltk.word_tokenize(text), n)\n",
        "\n",
        "for grams in pentagrams:\n",
        "    print(grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba24e787-48d7-4ec7-aff4-63962a65366f",
      "metadata": {
        "id": "ba24e787-48d7-4ec7-aff4-63962a65366f"
      },
      "source": [
        "## SECTION 2.4: Collocations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fa2163-1b50-4010-ad6f-ca1aa8b99dc5",
      "metadata": {
        "id": "f9fa2163-1b50-4010-ad6f-ca1aa8b99dc5"
      },
      "source": [
        "A collocation is a sequence of words that occur together unusually often."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79bbb7a3-b0e6-4f29-95d7-63f097679beb",
      "metadata": {
        "id": "79bbb7a3-b0e6-4f29-95d7-63f097679beb"
      },
      "source": [
        "`nltk.collocations` can help identifying phrases that act like single words.\n",
        "\n",
        "In the example below, bi-grams are paired with a \"more likely to occur\" score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "67acda6a-109e-4939-b0aa-9861e4f246b8",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67acda6a-109e-4939-b0aa-9861e4f246b8",
        "outputId": "a8d9f098-1a10-46db-a51a-02331a663719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New [('York', 4634.968955894195), ('Orleans', 611.6951040864856), ('England', 557.5789255397682), ('Jersey', 265.2781409189113), ('Testament', 182.6595658588261)]\n"
          ]
        }
      ],
      "source": [
        "import nltk.collocations\n",
        "import nltk.corpus\n",
        "import collections\n",
        "\n",
        "bgm = nltk.collocations.BigramAssocMeasures()\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(\n",
        "    nltk.corpus.brown.words()\n",
        ")\n",
        "scored = finder.score_ngrams(bgm.likelihood_ratio)\n",
        "\n",
        "# Group bigrams by first word in bigram\n",
        "prefix_keys = collections.defaultdict(list)\n",
        "for key, scores in scored:\n",
        "    prefix_keys[key[0]].append((key[1], scores))\n",
        "\n",
        "# Sorted key bigrams by strongest association\n",
        "for key in prefix_keys:\n",
        "    prefix_keys[key].sort(key = lambda x: -x[1])\n",
        "\n",
        "print('New', prefix_keys['New'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932298d3-c53f-4070-9634-5b379368cf16",
      "metadata": {
        "id": "932298d3-c53f-4070-9634-5b379368cf16"
      },
      "source": [
        "## SECTION 2.5: Term Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d6a18ae3-b36a-43d9-b5ee-2191405903ea",
      "metadata": {
        "tags": [],
        "id": "d6a18ae3-b36a-43d9-b5ee-2191405903ea"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "45e34cab-c0c9-40d7-95c9-bc388ef60bb1",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45e34cab-c0c9-40d7-95c9-bc388ef60bb1",
        "outputId": "8cf36b59-99d3-4597-afcd-4d8437e65bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 26\n",
            "a 14\n",
            "and 10\n",
            "their 7\n",
            "of 6\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "\n",
        "'''\n",
        "for line in gzip.open(\"F:/EDGE/LearningAI/sample_text.txt\", 'rt'):\n",
        "    data.extend(line.strip().split())\n",
        "'''\n",
        "\n",
        "for line in open(\"sample_text.txt\", 'rt'):\n",
        "    data.extend(line.strip().split())\n",
        "\n",
        "'''\n",
        "# Open the file and read its contents\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the file and split the text into words\n",
        "    words = file.read().split()\n",
        "'''\n",
        "\n",
        "counts = Counter(data)\n",
        "\n",
        "sorted_counts = sorted(list(counts.items()), key=lambda x:x[1],\n",
        "                       reverse=True)\n",
        "\n",
        "for word, count in sorted_counts[:5]:\n",
        "    print(word, count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "14ed0630-13ee-43d7-a0d7-a0c443de0505",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ed0630-13ee-43d7-a0d7-a0c443de0505",
        "outputId": "c8506769-62dd-4549-d39f-3e21552c09f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 26), ('a', 14), ('and', 10), ('their', 7), ('of', 6)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "all_words = nltk.FreqDist(data)\n",
        "print(all_words.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2737b6f-7b92-48c7-a086-22c98af18b90",
      "metadata": {
        "id": "c2737b6f-7b92-48c7-a086-22c98af18b90"
      },
      "source": [
        "## SECTION 2.6: 3 Ways to Remove Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be64ba3-0f89-4d39-9f5b-7affd9e2abbc",
      "metadata": {
        "id": "6be64ba3-0f89-4d39-9f5b-7affd9e2abbc"
      },
      "source": [
        "**(1/3) Remove the most common 100 words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d2f99ee5-e82b-4df8-815d-6e04fcd76855",
      "metadata": {
        "tags": [],
        "id": "d2f99ee5-e82b-4df8-815d-6e04fcd76855"
      },
      "outputs": [],
      "source": [
        "stopwords = set(\n",
        "    [\n",
        "        word for word,\n",
        "        count in sorted_counts[:100]\n",
        "    ]\n",
        ")\n",
        "\n",
        "clean_data = []\n",
        "\n",
        "for word in data:\n",
        "    if word not in stopwords:\n",
        "        clean_data.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438ae6fd-d8c1-4235-8879-f64decb54cb8",
      "metadata": {
        "id": "438ae6fd-d8c1-4235-8879-f64decb54cb8"
      },
      "source": [
        "**(2/3) Use nltk predefined stopwords:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzYwN4_ieS6U",
        "outputId": "2527775f-63d9-486d-e94f-473568875b91"
      },
      "id": "OzYwN4_ieS6U",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c3e6907f-1d41-4aa3-ba3a-15e344e7d192",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3e6907f-1d41-4aa3-ba3a-15e344e7d192",
        "outputId": "e13a91c3-a670-422b-b559-0026e12c26ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stopwords: 179\n",
            "First five stop words: ['i', 'me', 'my', 'myself', 'we']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "\n",
        "print('Number of stopwords: %d' % len(nltk_stopwords))\n",
        "print('First five stop words: %s' % list(nltk_stopwords)[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cec0981-6129-4f24-8557-ab2bd9ed31fb",
      "metadata": {
        "id": "5cec0981-6129-4f24-8557-ab2bd9ed31fb"
      },
      "source": [
        "**(3/3) Use spaCy predefined stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6909c612-c68a-4b9a-892e-6356f3e45d95",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6909c612-c68a-4b9a-892e-6356f3e45d95",
        "outputId": "7d312813-e908-42cc-faa0-9f45c2dcdad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stopwords: 326\n",
            "First five stop words: ['see', 'thus', 'would', 'her', '‘ll']\n"
          ]
        }
      ],
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
        "print('First five stop words: %s' % list(spacy_stopwords)[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1159f874-6ca0-4897-a837-a6f951decd28",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1159f874-6ca0-4897-a837-a6f951decd28",
        "outputId": "ea2333be-7774-481c-afa1-913c8f07efce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GENERATED', 'CHATGPT', 'time', ',', 'remote', 'island', 'nestled', 'heart', 'Pacific', ',', 'adventurer', 'named', 'Alex', 'found', 'stranded', 'unexpected', 'shipwreck', '.', 'island', ',', 'lush', 'vibrant', 'foliage', 'adorned', 'pristine', 'beaches', ',', 'like', 'paradise', 'glance', '.', ',', 'Alex', 'soon', 'realized', 'beauty', 'concealed', 'challenges', 'lay', 'ahead', '.', 'days', ',', 'Alex', 'scoured', 'shoreline', 'salvageable', 'items', 'shipwreck', '.', 'debris', ',', 'found', 'crates', 'canned', 'food', ',', 'tattered', 'functional', 'tent', ',', 'waterproof', 'box', 'containing', 'matches', '.', 'resources', ',', 'established', 'small', 'camp', 'near', 'freshwater', 'stream', ',', 'ensuring', 'basic', 'needs', 'met', '.', 'Days', 'turned', 'weeks', ',', 'Alex', 'survival', 'instincts', 'kicked', '.', 'began', 'exploring', 'island', 'interior', ',', 'learning', 'identify', 'edible', 'plants', 'honing', 'fishing', 'skills', '.', 'way', ',', 'encountered', 'peculiar', 'wildlife', ',', 'friendly', '.', 'encounter', 'added', 'understanding', 'island', 'ecosystem', '.', 'Loneliness', 'soon', 'companion', ',', 'combat', ',', 'Alex', 'crafted', 'makeshift', 'journal', '.', 'documented', 'experiences', ',', 'pouring', 'hopes', 'fears', 'weathered', 'pages', '.', 'journal', 'confidant', ',', 'way', 'preserve', 'sanity', 'solitude', 'island', '.', 'Amidst', 'challenges', ',', 'Alex', 'discovered', 'hidden', 'cave', 'adorned', 'ancient', 'markings', 'artifacts', ',', 'hinting', 'island', 'mysterious', 'past', '.', 'markings', 'told', 'story', 'lost', 'civilization', 'called', 'island', 'home', '.', 'Intrigued', ',', 'Alex', 'delved', 'deeper', 'cave', ',', 'unearthing', 'secrets', 'forever', 'change', 'perspective', 'island', '.', 'end', ',', 'Alex', 'journey', 'stranded', 'island', 'survival', 'self', '-', 'discovery', 'resilience', 'human', 'spirit', '.', 'repaired', 'makeshift', 'raft', 'set', 'sail', 'horizon', ',', 'carried', 'island', 'lessons', ',', 'forever', 'changed', 'adventure', 'fate', 'cast', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Combine the list of words into a single string\n",
        "data_str = ' '.join(data)\n",
        "\n",
        "# Process your text using spaCy\n",
        "doc = nlp(data_str)\n",
        "\n",
        "# Extract tokens, excluding stop words\n",
        "tokens = [token.text for token in doc\n",
        "          if not nlp.vocab[token.text].is_stop]\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d771cce9-8142-4adb-88a3-52d44f1b361e",
      "metadata": {
        "id": "d771cce9-8142-4adb-88a3-52d44f1b361e"
      },
      "source": [
        "## SECTION 2.7: TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ac572f-6ce2-4b89-a9d9-cb941be5d3d7",
      "metadata": {
        "id": "03ac572f-6ce2-4b89-a9d9-cb941be5d3d7"
      },
      "source": [
        "TF-IDF reflects how important a word is to a document in a corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00446cc4-afd9-4d70-97e1-f0bb8e7c1b17",
      "metadata": {
        "id": "00446cc4-afd9-4d70-97e1-f0bb8e7c1b17"
      },
      "source": [
        "$$W_{x,y}=tf_{x,y}\\times log(\\frac{N}{df_{x}})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3376846-3620-4e63-95ac-43b838bcf627",
      "metadata": {
        "id": "d3376846-3620-4e63-95ac-43b838bcf627"
      },
      "source": [
        "Here,\n",
        "> $W_{x,y}$ is the TF-IDF, i.e. the term $x$ within the document $y$\n",
        "\n",
        "> $tf_{x,y}$ is the frequency of $x$ in $y$\n",
        "\n",
        "> $df_{x}$ is the number of documents containing $x$\n",
        "\n",
        "> $N$ is the total number of documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6884c68-e441-4a0a-802e-877c40c1f9d2",
      "metadata": {
        "id": "a6884c68-e441-4a0a-802e-877c40c1f9d2"
      },
      "source": [
        "TF-IDF of common words is zero."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e1078f-cd80-43f1-8f19-b2a6c25881fe",
      "metadata": {
        "id": "b3e1078f-cd80-43f1-8f19-b2a6c25881fe"
      },
      "source": [
        "**(1/3) TF-IDF from Scratch:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "83fa4cef-c4e8-43f7-8c92-fe96e3f91759",
      "metadata": {
        "tags": [],
        "id": "83fa4cef-c4e8-43f7-8c92-fe96e3f91759"
      },
      "outputs": [],
      "source": [
        "def computeTF(wordDict, bow):\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bowCount)\n",
        "    return tfDict\n",
        "\n",
        "def computeIDF(docList):\n",
        "    import math\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "\n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N / float(val))\n",
        "\n",
        "    return idfDict\n",
        "\n",
        "def computeTFIDF(tfBow, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        tfidf[word] = val*idfs[word]\n",
        "    return tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449ba9e9-6ebf-4bb7-a815-457f83cb4099",
      "metadata": {
        "id": "449ba9e9-6ebf-4bb7-a815-457f83cb4099"
      },
      "source": [
        "**(2/3) TF-IDF using Sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3cd70c94-1b7a-4117-b39f-a41599c08733",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cd70c94-1b7a-4117-b39f-a41599c08733",
        "outputId": "5b76fbfa-42ca-420f-8a73-304e1cc58155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5)\t0.42471718586982765\n",
            "  (0, 4)\t0.30218977576862155\n",
            "  (0, 1)\t0.30218977576862155\n",
            "  (0, 3)\t0.30218977576862155\n",
            "  (0, 0)\t0.42471718586982765\n",
            "  (0, 6)\t0.6043795515372431\n",
            "  (1, 2)\t0.42471718586982765\n",
            "  (1, 7)\t0.42471718586982765\n",
            "  (1, 4)\t0.30218977576862155\n",
            "  (1, 1)\t0.30218977576862155\n",
            "  (1, 3)\t0.30218977576862155\n",
            "  (1, 6)\t0.6043795515372431\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "D1 = \"The car is driven on the road.\"\n",
        "D2 = \"The truck is driven on the highway.\"\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "response = vectorizer.fit_transform([D1, D2])\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "44ce1c74-3131-4592-8990-dc93fdf652c6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ce1c74-3131-4592-8990-dc93fdf652c6",
        "outputId": "68d31193-dcb5-4cf6-a6c5-a010b20d9b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ],
      "source": [
        "pprint(list(enumerate(vectorizer.get_feature_names())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e731a086-0506-4727-8a9b-12e58ea3cbe6",
      "metadata": {
        "id": "e731a086-0506-4727-8a9b-12e58ea3cbe6"
      },
      "source": [
        "`fit` learns vocabulary and idf from the training set.\n",
        "\n",
        "`transform` transforms documents to document-term matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef5321e-28da-43c1-a0d5-bee25812b0db",
      "metadata": {
        "id": "bef5321e-28da-43c1-a0d5-bee25812b0db"
      },
      "source": [
        "**(3/3) TF_IDF using Sklearn + StopWords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fa5add55-6859-4b64-a846-3a2aead80efc",
      "metadata": {
        "tags": [],
        "id": "fa5add55-6859-4b64-a846-3a2aead80efc"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "9434e556-ea5e-4131-9c3e-e61c6fc62683",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "9434e556-ea5e-4131-9c3e-e61c6fc62683",
        "outputId": "dc8a3912-521b-4094-fbed-4986817b7d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3)\t0.6316672017376245\n",
            "  (0, 1)\t0.4494364165239821\n",
            "  (0, 0)\t0.6316672017376245\n",
            "  (1, 2)\t0.6316672017376245\n",
            "  (1, 4)\t0.6316672017376245\n",
            "  (1, 1)\t0.4494364165239821\n",
            "[(0, 'car'), (1, 'driven'), (2, 'highway'), (3, 'road'), (4, 'truck')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npprint(\\n    list(\\n        enumerate(\\n            vectorizer.get_feature_names()\\n        )\\n    )\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "D1 = \"The car is driven on the road.\"\n",
        "D2 = \"The truck is driven on the highway.\"\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "response = vectorizer.fit_transform([D1, D2])\n",
        "\n",
        "print(response)\n",
        "\n",
        "# Print the feature names\n",
        "# feature_names = vectorizer.get_feature_names()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "pprint(list(enumerate(feature_names)))\n",
        "'''\n",
        "pprint(\n",
        "    list(\n",
        "        enumerate(\n",
        "            vectorizer.get_feature_names()\n",
        "        )\n",
        "    )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39558322-0d20-4e5b-a42a-9768ddb9d90a",
      "metadata": {
        "id": "39558322-0d20-4e5b-a42a-9768ddb9d90a"
      },
      "source": [
        "# SECTION 3: Vector Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0284a6-60af-4e53-8ebb-3d6fc8135cc3",
      "metadata": {
        "id": "db0284a6-60af-4e53-8ebb-3d6fc8135cc3"
      },
      "source": [
        "## SECTION 3.1: Popular Word Embedding Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b85a892-1d8e-49e1-a67b-919a205b41a0",
      "metadata": {
        "id": "4b85a892-1d8e-49e1-a67b-919a205b41a0"
      },
      "source": [
        "1. skip-gram\n",
        "2. Continuous Bag of Words (CBOW)\n",
        "3. Word2Vec (2013) by Google - trains using CBOW and Skip-Gram together\n",
        "4. Global Vectors for Word Representations (GloVe) by a team at Stanford University\n",
        "5. fasttext by Facebook AI group"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4fda49-d753-4d47-9b4f-7a6301d4ada5",
      "metadata": {
        "id": "bf4fda49-d753-4d47-9b4f-7a6301d4ada5"
      },
      "source": [
        "## SECTION 3.2: Word Embeddings with Spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3zmt9Cwepno",
        "outputId": "aae27c83-8c5b-499a-b4e9-db1d4ff2aad6"
      },
      "id": "v3zmt9Cwepno",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-19 17:26:32.211954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "877f417c-6ac5-4e94-9059-13ebdeb5ba99",
      "metadata": {
        "tags": [],
        "id": "877f417c-6ac5-4e94-9059-13ebdeb5ba99"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "dda7fc4b-7aae-4e9c-96e4-e5ea0d4086b4",
      "metadata": {
        "tags": [],
        "id": "dda7fc4b-7aae-4e9c-96e4-e5ea0d4086b4"
      },
      "outputs": [],
      "source": [
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = \\\n",
        "lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "man = nlp.vocab['man'].vector\n",
        "woman = nlp.vocab['woman'].vector\n",
        "queen = nlp.vocab['queen'].vector\n",
        "king = nlp.vocab['king'].vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688e34f4-9720-4643-bcde-a5986fb00e79",
      "metadata": {
        "id": "688e34f4-9720-4643-bcde-a5986fb00e79"
      },
      "source": [
        "We now need to find the closest vector in the vocabulary to the result of \"king - man + woman\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "be7b0a34-ae11-4ed2-8a6b-806343a79d49",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be7b0a34-ae11-4ed2-8a6b-806343a79d49",
        "outputId": "c9053c5a-2f82-400c-cf95-5c623be383c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object <genexpr> at 0x7ce5447fa340>\n"
          ]
        }
      ],
      "source": [
        "maybe_queen = king - man + woman\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    if not word.has_vector:\n",
        "        continue\n",
        "\n",
        "    similarity = cosine_similarity(maybe_queen, word.vector)\n",
        "    computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(\n",
        "    computed_similarities, key=lambda item: -item[1]\n",
        ")\n",
        "print(w[0].text for w in computed_similarities[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 3.3: FastText"
      ],
      "metadata": {
        "id": "qhR__luNfNjw"
      },
      "id": "qhR__luNfNjw"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JhgxVi4f1Fx",
        "outputId": "65cefa19-025e-494b-b8a9-1762d484a9f6"
      },
      "id": "5JhgxVi4f1Fx",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/68.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199769 sha256=dbd67c01ee36a79f30250e76f00a25aa2c4623abb3e024bfee31475ede491a29\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "cf477585-fff9-499c-aa63-a6c64f351695",
      "metadata": {
        "id": "cf477585-fff9-499c-aa63-a6c64f351695"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "# skipgram model:\n",
        "skipgram_model = fasttext.train_unsupervised(\n",
        "    'sample_text.txt', model='skipgram'\n",
        ")\n",
        "cbow_model = fasttext.train_unsupervised(\n",
        "    'sample_text.txt', model='cbow'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model.get_word_vector(\"environment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InDUreIefovn",
        "outputId": "1a77bcec-de79-460d-944c-752468fe864d"
      },
      "id": "InDUreIefovn",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.74022898e-05,  2.80962646e-04,  5.05377029e-05,  2.98797422e-05,\n",
              "       -3.74962663e-04, -9.13969197e-05,  1.70033003e-04, -2.15993365e-04,\n",
              "        2.53577076e-04,  2.07336037e-04, -5.01854694e-04, -1.13218390e-04,\n",
              "       -4.86230914e-04, -4.21434845e-04, -6.72254770e-04,  1.86717123e-04,\n",
              "        8.09436242e-05,  4.61690870e-05,  4.64938296e-07, -3.91224137e-04,\n",
              "        2.23598647e-04,  1.53131492e-04, -6.70746667e-05, -5.31157886e-04,\n",
              "       -5.16778273e-05,  6.97572104e-05, -5.54304221e-04,  1.52136490e-04,\n",
              "        4.62088610e-05, -3.10951946e-05, -1.87138925e-04, -2.42755210e-04,\n",
              "        7.47237136e-05,  2.55463645e-04, -4.95463784e-04,  7.32286353e-05,\n",
              "       -4.98533947e-04,  2.05337725e-04, -9.69687535e-05,  5.07802295e-04,\n",
              "        1.31217006e-04,  6.31086819e-04, -3.81859514e-04,  1.86736113e-04,\n",
              "       -1.88688253e-04,  5.29979821e-04, -1.95597502e-04, -5.07256074e-04,\n",
              "        1.63958714e-04,  5.71078272e-04, -7.01919897e-04,  2.38064429e-04,\n",
              "       -3.82221682e-04,  3.59700469e-04,  1.01397352e-04,  1.29188295e-04,\n",
              "        2.12687242e-04,  1.00273945e-04,  2.94364319e-04, -2.41203161e-04,\n",
              "       -6.62664170e-05,  2.37164189e-04, -9.31511531e-05, -3.30855721e-04,\n",
              "        3.63856707e-05,  1.82283708e-04, -4.30686167e-04, -2.30004909e-04,\n",
              "       -2.65654253e-05,  2.67524680e-04,  5.07702898e-05,  6.47476627e-05,\n",
              "       -1.73992317e-04, -4.31276421e-04,  5.05248492e-04,  5.28360950e-04,\n",
              "        1.41022610e-04,  1.79473864e-04, -7.78094272e-06, -1.49428830e-04,\n",
              "        9.97536772e-05, -7.90577833e-05,  5.26437361e-04, -5.76729835e-05,\n",
              "       -3.50636023e-04,  6.99190059e-05,  2.17894878e-04,  2.87960574e-04,\n",
              "       -4.61622185e-05, -3.96804498e-05, -2.82026682e-04,  4.23527781e-05,\n",
              "        2.24428601e-04, -8.71937256e-04,  5.21157112e-04, -1.42678284e-04,\n",
              "        5.31567901e-04,  6.86882049e-05, -6.00434490e-04, -2.77697924e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing your model:**\n",
        "\n",
        "Check nearest neighbours -"
      ],
      "metadata": {
        "id": "_XwqEG4DgK_d"
      },
      "id": "_XwqEG4DgK_d"
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model.get_nearest_neighbors('island')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9TPfQyTgTsM",
        "outputId": "14ffe12d-38e5-4cd1-c37c-1d0942b124ef"
      },
      "id": "-9TPfQyTgTsM",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.6100154519081116, 'and'), (0.146626815199852, 'a'), (0.09915514290332794, 'they'), (0.09666965901851654, 'of'), (0.09045328944921494, 'the'), (0.08943742513656616, '</s>'), (0.08328405767679214, 'Alex'), (0.07356219738721848, 'their')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model.get_nearest_neighbors('island')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kng9Pm-gX9D",
        "outputId": "b11227c4-8bc9-4353-edb9-b97ad6627605"
      },
      "id": "1Kng9Pm-gX9D",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.6045956015586853, 'and'), (0.14456523954868317, 'a'), (0.08874776214361191, '</s>'), (0.08798855543136597, 'they'), (0.08178038150072098, 'of'), (0.06840469688177109, 'Alex'), (0.06475301086902618, 'the'), (0.025664031505584717, 'their')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "itil",
      "language": "python",
      "name": "itil"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}