{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99e8b4f-24bd-49e2-9567-0aed6d5359da",
   "metadata": {},
   "source": [
    "# 1. What constitutes a probability measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079338c-1a2a-4327-b209-cd7fe9ee26bd",
   "metadata": {},
   "source": [
    "A *probability measure* (or *probability distribution*) $P$ on the sample space ($S$, <span style=\"font-family: 'cursive';\">S</span>) is a real-valued function defined on the collection of events that <span style=\"font-family: 'cursive';\">S</span> satisifes the following axioms:\n",
    "1. $P(A) >= 0$ for every event $A$\n",
    "2. $P(S) = 1$\n",
    "3. If ${A_i:i\\in I}$ is a countable, pairwise disjoint collection of events then \n",
    "$$P(\\bigcup_{i\\in I}A_i)=\\sum_{i\\in I}P(A_i)$$\n",
    "\n",
    "Source: https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/02%3A_Probability_Spaces/2.03%3A_Probability_Measures#:~:text=satisfies%20certain%20axioms.-,Definition,P(S)%3D1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa67b0d-3bed-4121-bedf-8892b946c0e8",
   "metadata": {},
   "source": [
    "# 2. Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd13096-09b9-4f6c-bb53-aa99810e5c35",
   "metadata": {},
   "source": [
    "P(A,B) = P(A)P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192c0d0-fb82-4398-94dd-6450aa53a9b1",
   "metadata": {},
   "source": [
    "# 3. Conditional Probabilty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbb145-33fc-447c-b4d7-5ae4ad4cbf47",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$P(A|B) = \\frac{P(A,B)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136f20-4d67-48ec-8c87-74f1bd7d0def",
   "metadata": {},
   "source": [
    "# 4. Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f5eef-7ec2-4463-b3db-9eb908742369",
   "metadata": {},
   "source": [
    "A *random variable*, usually written *X*, is a variable whose possible values are numerical outcomes of a random phenomenon.\n",
    "\n",
    "Source: http://www.stat.yale.edu/Courses/1997-98/101/ranvar.htm#:~:text=Discrete%20Random%20Variables,then%20it%20must%20be%20discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3282e-c79e-452e-a3dc-cf746e32283e",
   "metadata": {},
   "source": [
    "## 4.1 Discrete Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a718e-6f85-48c8-9481-e1f089699c93",
   "metadata": {},
   "source": [
    "A *discrete random variable* is one which may take on only a countable number of distinct values such as 0,1,2,3,4,........ Discrete random variables are usually (but not necessarily) counts. If a random variable can take only a finite number of distinct values, then it must be discrete. Examples of discrete random variables include the number of children in a family, the number of patients in a doctor's surgery, the number of defective light bulbs in a box of ten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc37f38-f5ac-489c-916a-1d79d7bbc155",
   "metadata": {},
   "source": [
    "## 4.2 Continuous Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6ad92-5df7-4269-a091-7acc78041175",
   "metadata": {},
   "source": [
    "A *continuous random variable* is one which takes an infinite number of possible values. Continuous random variables are usually measurements. Examples include height, weight, the amount of sugar in an orange, the time required to run a mile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915ed24-685f-4d1e-95cc-359776a8fa07",
   "metadata": {},
   "source": [
    "# 5. Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1822d-822d-4f65-bd8c-3e1ec1f1c443",
   "metadata": {},
   "source": [
    "A language model uses machine learning to conduct a probability distribution over words used to predict the most likely next word in a sentence based on the previous entry. Language models learn from text and can be used for producing original text, predicting the next word in a text, speech recognition, optical character recognition and handwriting recognition.\n",
    "\n",
    "Source: https://builtin.com/data-science/beginners-guide-language-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b56f5-e6a7-4f67-ab17-a04b7151029e",
   "metadata": {},
   "source": [
    "# 6. Maximum Likelihood Estimation for Binomials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b7cb7-e66e-4933-ae49-a070923e1ade",
   "metadata": {},
   "source": [
    "Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution:\n",
    "$$y\\simeq Bin(n,p)$$\n",
    "Then, the maximum likelihood estimator of $p$ is\n",
    "$$\\hat{p} = \\frac{y}{n}$$\n",
    "\n",
    "Source: https://statproofbook.github.io/P/bin-mle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b11fb-af55-4062-b47e-53dd049f2585",
   "metadata": {},
   "source": [
    "# 7. Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83458477-73b6-4f9b-b2c6-f62f78489ea4",
   "metadata": {},
   "source": [
    "**Markov chain** is a mathematical chain of events or states that describe the probability of the events that might occur in the future, based on the current state and not the previous states. It is a *stochastic model* that predicts the future based on the present state.\n",
    "\n",
    "In a Markov Chain, each state can be represented as a set of discrete steps. Each state has its own probability of transitioning to every other state. This may be represented by a weighted connected graph or by a transition matrix.\n",
    "\n",
    "Source: https://www.educative.io/answers/introduction-to-markov-chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f43092-e697-49c7-bdd3-6e1931da417c",
   "metadata": {},
   "source": [
    "# 8. Markov Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb61500-1c7a-416d-9e87-8bc0ff30cc7f",
   "metadata": {},
   "source": [
    "The **Markov assumption**, a tenet named in honor of the Russian mathematician Andrey Markov, is a central idea in the sphere of probabilistic models, and more so in Markov processes. At its core, the Markov assumption proposes that the future state of a process relies solely on the current state by disregarding the journey to the current state. This attribute is commonly known as the \"memoryless\" aspect or \"absence of memory\" disregarding in Markov processes.\n",
    "\n",
    "Source: https://www.educative.io/answers/what-is-the-markov-assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9789f9-3e0f-4e20-b559-cf2617fc2f2f",
   "metadata": {},
   "source": [
    "# 9. Why is word sparcity an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72cd42-dae2-4497-9557-bd0fdddd2f32",
   "metadata": {},
   "source": [
    "Word sparsity can be an issue in natural language processing (NLP) and machine learning tasks for several reasons:\n",
    "\n",
    "1. **Data Scarcity**: In many NLP applications, you're working with large vocabularies or feature spaces. When building models based on text data, it's common to have a vast number of unique words or tokens in a corpus. However, not all words appear frequently in the data. This leads to data sparsity, where many words occur only a few times or even just once in your dataset. Sparse data can be challenging for statistical models because they lack sufficient examples to learn meaningful patterns.\n",
    "\n",
    "2. **Reduced Model Generalization**: Sparse data can lead to overfitting. When a model encounters rare words that it has only seen a few times during training, it may fit to the noise in the data rather than capturing the true underlying patterns. This can result in poor generalization to new, unseen data.\n",
    "\n",
    "3. **Increased Model Complexity**: Dealing with sparse data often requires more complex models. For instance, if you're using a bag-of-words representation where each unique word is a feature, you might end up with a high-dimensional feature space. This can increase model complexity and the computational resources required for training and inference.\n",
    "\n",
    "4. **Loss of Information**: Rare words or infrequent features may carry valuable information. In tasks like sentiment analysis or topic modeling, uncommon words can be strong indicators of sentiment or topic. When you discard or downweight these features due to their rarity, you lose potentially important information.\n",
    "\n",
    "5. **Efficiency Challenges**: Sparse data can be computationally inefficient to process and store. In large-scale NLP applications, it can lead to performance bottlenecks and increased memory requirements.\n",
    "\n",
    "To address issues related to word sparsity, NLP practitioners often use techniques like:\n",
    "\n",
    "- **Text Preprocessing**: Removing or reducing word sparsity by applying techniques like stemming, lemmatization, or removing stop words.\n",
    "- **Feature Engineering**: Using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to weight words based on their importance.\n",
    "- **Dimensionality Reduction**: Applying techniques like Principal Component Analysis (PCA) or Truncated SVD (Singular Value Decomposition) to reduce the dimensionality of sparse feature spaces.\n",
    "- **Word Embeddings**: Using word embeddings (e.g., Word2Vec, GloVe) to represent words as dense vectors in a lower-dimensional space. This not only reduces sparsity but also captures semantic relationships between words.\n",
    "- **Data Augmentation**: Expanding the dataset by generating more data through techniques like back-translation or synonym replacement.\n",
    "- **Transfer Learning**: Leveraging pre-trained models like BERT or GPT, which have learned from large corpora and can handle word sparsity more effectively.\n",
    "\n",
    "Addressing word sparsity is crucial for improving the performance of NLP models, especially in tasks where capturing subtle linguistic nuances is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e408866-23d7-4808-aaa0-71aa2ab928f9",
   "metadata": {},
   "source": [
    "# Latex Styling Used in this Markdown Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da61673-dadc-498d-8f04-098d39dbf962",
   "metadata": {},
   "source": [
    "1. Set notation, union: https://latex-tutorial.com/union-latex/\n",
    "2. Summation: https://www.physicsread.com/latex-summation/\n",
    "3. Set notations: https://www.geeksforgeeks.org/set-notations-in-latex/\n",
    "4. Equations: https://www.fabriziomusacchio.com/blog/2021-08-10-How_to_use_LaTeX_in_Markdown/\n",
    "5. Similarly Equivalent: https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itil",
   "language": "python",
   "name": "itil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
